---
title: "Eltecon Data Science Course by Emarsys"
subtitle: "Introduction to Prediction"
author: "Tamás Koncz"
date: "Novebmer 11, 2020"
output:
  beamer_presentation:
    colortheme: dolphin
    fonttheme: structurebold
    theme: AnnArbor
    # toc: true
    slide_level: 2
classoption: "aspectratio=169"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(out.width = '65%')
knitr::opts_chunk$set(fig.align = 'center')
# knitr::opts_knit$set(root.dir = '..')

library(knitr)
library(data.table)
library(magrittr)
library(ggplot2)
library(ggtext)
```

## Homeworks from last week

* Im Seongwon - Kim Yeonggyeong
* Szőnyi Máté - Tran, Dung
* Sármány Áron - Schmall Róbert

## Goal of the lesson

- Understand what _prediction_ is, and how it differs from _causal inference_
- Try basic `R` commands for fitting & evaluating simple regression and classification models

# What is prediction?

**Assumption:**
$$Y \approx f(X) + \epsilon$$

where:

- $Y$ is your target variable
- $X$ are your predictors
- $f()$: is the assumed relationship between $X$ and $Y$
- $\epsilon$: is the irreducible error

**Prediction is**:

- estimating $f()$ based on the available observations ($X$) ...
- **... to minimize the error $E(Y-\hat{Y})$**

# Regression

## When to use linear regression?

When:

* We want to estimate a quantitative target (e.g. price)
* Assuming an approximate linear relationship between $X$ and $Y$

Simple linear regression formula:

$$ \hat{Y} = \beta_0 + \beta_1 X $$

## RMSE

* Root Mean Square Error (RMSE)
* It is the standard deviation of the residuals i.e. prediction errors
* RMSE penalizes the model for large errors

$$ RMSE = \sqrt{\frac{\sum_{i=1}^n (\hat{y_i}-y_i)^2}{n}} $$

## (So what is Machine Learning?)

_“A computer program is said to **learn from experience** E with respect to some class of tasks T and performance measure P, **if its performance** at tasks in T, as measured by P, **improves with experience** E.”_
(Tom Mitchell)

## Bike rental - the dataset

* Source: [Kaggle](https://www.kaggle.com/c/bike-sharing-demand/overview/description)
* Goal: Predict the total count of bikes rented during each hour

\small
```{r}
bike_sharing_train <- fread("bike_sharing_train.csv")

knitr::kable(head(
  bike_sharing_train[, -c("weather_3", "weather_4", "season_3", "season_4")]
))
```
\normalsize

## Bike rental - variables

\small
```{r}
fread("bike_sharing_data_fields.csv", sep = ";") %>%
    .[field %in% c("count", "season", "holiday", "workingday",
                   "weather", "temp", "atemp", "humidity", "windspeed")
    ] %>%
    knitr::kable()
```
\normalsize

## Bike rental - minimal model

```{r, echo = TRUE}
bike_sharing_min_model <- glm(
  formula = count ~ windspeed,
  family = gaussian,
  data = bike_sharing_train
)
```


## Bike rental - minimal model

\tiny
```{r, echo = TRUE}
summary(bike_sharing_min_model)
```
\normalsize

## Bike rental - minimal model - coefficients

```{r, echo = TRUE}
str(bike_sharing_min_model$coefficients)
```

```{r, echo = TRUE}
intercept <- bike_sharing_min_model$coefficients[1]
slope <- bike_sharing_min_model$coefficients[2]
```

## Bike rental - minimal model - predictive fit

```{r}
ggplot(bike_sharing_train, aes(x = temp, y = count)) +
    geom_point() +
    geom_abline(intercept = intercept, slope = slope, color = "red") +
    ggtitle("Minimal model fit for bike rentals") +
    theme_classic() +
    theme(text = element_text(size = 20))
```

## Bike rental - minimal model - prediction error

```{r, echo = TRUE}
predictions_min_model <- predict.glm(
  bike_sharing_min_model, newdata = bike_sharing_train
)

predictions_min_model[1:5]
bike_sharing_train[1:5, count]
```

## Bike rental - minimal model - prediction error

```{r, echo = TRUE}
calculateRMSE <- function(actual, predictions) {
  sqrt(mean((actual - predictions) ^ 2))
}
```

```{r, echo = TRUE}
bike_sharing_min_model_rmse <- calculateRMSE(
  actual = bike_sharing_train[, count],
  predictions = predictions_min_model
)

bike_sharing_min_model_rmse
```

## Bike rental - improving predictions

\small
```{r, echo = TRUE}
bike_sharing_2nd_model <- glm(
  formula = count ~ windspeed + temp,
  family = gaussian, data = bike_sharing_train
)

predictions_2nd_model <- predict.glm(
  bike_sharing_2nd_model, newdata = bike_sharing_train
)

calculateRMSE(
  actual = bike_sharing_train[, count],
  predictions = predictions_2nd_model
)
```
\normalsize

## Practice time

* Task: improve the model to be as accurate as possible!
* Share your regression formula + achieved RMSE in [Socrative](https://www.socrative.com/#login)!
* You have 20 minutes - feel free to take a break if needed.


## Practice time

"SOLUTION"

## Don't worry about Confounding!

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics('confounding.png')
```

## Don't worry about Confounding!

```{r}
ggplot(bike_sharing_train, aes(x = temp, y = atemp)) +
    geom_point() +
    ggtitle("Temperature vs 'Feels Like' Temperature") +
    theme_classic() +
    theme(text = element_text(size = 20))
```

<!-- https://stats.stackexchange.com/questions/271694/confounding-variables-in-machine-learning-predictions -->

<!-- model explainability -->
<!-- example: emarsys BPS -->
<!-- retraining -> validity -->
<!-- TODO -->

# Classification

## Binary classification

* Binary: target can take on two values (0 or 1)
* Typical example is predicting if an event is happening or not
* Examples:
  * Patient has a medical condition or not
  * Loan will be repaid in full
  * User will make a purchase in the next 30 days (BPS - Buying Probability Score)

## Question time

* __Why should we not use linear regression for binary classification__?
* Please record your answers in [Socrative](https://www.socrative.com/#login)!
* You have 5 minutes

## Logistic regression

* A linear model makes continuous predictions that are unbounded.
* In classification, we are interested in the probability of an outcome occurring
* So we want **predictions that are bounded between 0 and 1.**

<!-- + Predicting binary outcomes with a linear model violates the assumption of normal residuals, distorting inferences made on regression coefficients.
 -->

$$ Pr(Y_i=1|X_i) = {\frac{exp(\beta_0 + \beta_1X_i)}{1 + exp (\beta_0 + \beta_1X_i)}} $$

## Logistic regression

TODO add chart sigmoid transformation

## The Titanic dataset


```{r, eval = FALSE, echo = TRUE}
install.packages("titanic")
library(titanic)
head(titanic_train)
```

\tiny
```{r}
library(titanic)
knitr::kable(head(titanic_train))
```
\normalsize

## Example - binary model

```{r, echo = TRUE}
model <- glm(
    Survived ~ Sex + Fare,
    data = titanic_train,
    family = binomial
)
```

## Making predictions

```{r, echo = TRUE}
predicted_prob <- predict.glm(model, newdata = titanic_train, type = "response")
predicted_prob[1:5]
```

## Predictive fit

```{r}
titanic_train %>%
    as.data.table() %>%
    copy() %>%
    .[, predicted_prob := predicted_prob] %>%
    ggplot(aes(x = Fare)) +
        geom_point(aes(y = Survived)) +
        geom_point(aes(y = predicted_prob), color = "red") +
        facet_wrap(.~Sex) +
        labs(
          title = "Model fit for Titanic survival",
          subtitle = "<span style = 'color: black;'>Actual</span> vs <span style = 'color: red;'>Predicted Probability</span>"
        ) +
        theme_classic() +
        theme(text = element_text(size = 20)) +
        theme(plot.subtitle = ggtext::element_markdown())
```

## Converting probabilities to predictions

```{r, echo = TRUE}
titanic_train[1:5, "Survived"]
```

__Using `cutoff = 0.5`:__

```{r, echo = TRUE}
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
predicted_class[1:5]
```

## Evaluating binary models - Accuracy

```{r, echo = TRUE}
calculateAccuracy <- function(actual, predicted) {
    N <- length(actual)
    accuracy <- sum(actual == predicted) / N

    return(accuracy)
}
```

```{r, echo = TRUE}
calculateAccuracy(titanic_train$Survived, predicted_class)
```

## Evaluating binary models - Confusion Matrix

```{r, echo = TRUE}
table(
  titanic_train$Survived,
  predicted_class,
  dnn = c("actual", "predicted")
)
```

## Practice time

* Task: improve the model to be as accurate as possible!
* Share your regression formula + achieved Accuracy & Confusion Matrix in [Socrative](https://www.socrative.com/#login)!
* You have 20 minutes - feel free to take a break if needed.

# Generalization performance

## Why do we care?

* It's easy to predict something we already know...
* Actually, it would be silly to build predictive models to predict what we already know!

* What we are after is **out-of-sample** performance

## Example

TODO

<!-- ## Example on the Bike Sharing dataset - simple vs overspecified model -->

<!-- ```{r, echo = TRUE} -->
<!-- simple_model <- glm(count ~ atemp, data = bike_sharing) -->

<!-- overfit_model <- glm(count ~ poly(atemp, 20), data = bike_sharing) -->
<!-- ``` -->

<!-- ## Simple model - Accuracy on our training data -->

<!-- ```{r, echo = TRUE} -->
<!-- simple_model_predictions <- predict.glm(simple_model, newdata = bike_sharing) -->
<!-- calculateRMSE(bike_sharing$count, simple_model_predictions) -->
<!-- ``` -->

<!-- ## Overspecified model - Accuracy on our training data -->

<!-- ```{r, echo = TRUE} -->
<!-- overfit_model_predictions <- predict.glm(overfit_model, newdata = bike_sharing) -->
<!-- calculateRMSE(bike_sharing$count, overfit_model_predictions) -->
<!-- ``` -->

<!-- ## Simple model - Predictive fit on our training data -->

<!-- ```{r} -->
<!-- bike_sharing %>% -->
<!--     as.data.table() %>% -->
<!--     copy() %>% -->
<!--     .[, predicted_prob := overfit_model_predictions] %>% -->
<!--     ggplot(aes(x = atemp)) + -->
<!--         geom_point(aes(y = count)) + -->
<!--         geom_point(aes(y = predicted_prob), color = "red") + -->
<!--         facet_wrap(.~Sex) + -->
<!--         labs( -->
<!--           title = "Model fit for Titanic survival", -->
<!--           subtitle = "<span style = 'color: black;'>Actual</span> vs <span style = 'color: red;'>Predicted Probability</span>" -->
<!--         ) + -->
<!--         theme_classic() + -->
<!--         theme(text = element_text(size = 20)) + -->
<!--         theme(plot.subtitle = ggtext::element_markdown()) -->
<!-- ``` -->

<!-- ## Overspecified model - Predictive fit on our training data -->

<!-- ```{r} -->
<!-- titanic_train %>% -->
<!--     as.data.table() %>% -->
<!--     copy() %>% -->
<!--     .[, predicted_prob := overfit_model_predicted_prob] %>% -->
<!--     ggplot(aes(x = Fare)) + -->
<!--         geom_point(aes(y = Survived)) + -->
<!--         geom_point(aes(y = predicted_prob), color = "red") + -->
<!--         facet_wrap(.~Sex) + -->
<!--         labs( -->
<!--           title = "Model fit for Titanic survival", -->
<!--           subtitle = "<span style = 'color: black;'>Actual</span> vs <span style = 'color: red;'>Predicted Probability</span>" -->
<!--         ) + -->
<!--         theme_classic() + -->
<!--         theme(text = element_text(size = 20)) + -->
<!--         theme(plot.subtitle = ggtext::element_markdown()) -->
<!-- ``` -->

<!-- ## Out of sample accuracy -->
<!-- ```{r} -->
<!-- titanic_test -->
<!-- ``` -->

# Homework

## Homework

* __Final project__: Have a high level outline of how you plan to answer your research question
* If you haven't finished the binary classification practice, do so
  * Show your best Logit formula!
  * Present your achieved Accuracy & Confusion Matrix!
* Presenters:
  * Alexandrov Dániel - Földesi Attila
  * Nguyen Thai Duong - Szentistványi János
  * Kovács Ádám - Nguyen Nam Son



