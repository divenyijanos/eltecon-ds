---
title: "Eltecon Data Science Course by Emarsys"
subtitle: "Measuring uncertainty"
author: "András Bérczi"
date: "October 14, 2020"
output:
  beamer_presentation:
    colortheme: dolphin
    fonttheme: structurebold
    theme: AnnArbor
    # toc: true
    slide_level: 2
header-includes:
   - \usepackage{animate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
# knitr::opts_knit$set(root.dir='..')

library(knitr)
library(data.table)
library(magrittr)
library(ggplot2)
library(gganimate)
library(magick)
library(ggtext)
library(flextable)
```

# Homeworks from last week

Nguyen Thai Duong - Szentistványi János

Kovács Ádám - Nguyen Nam Son

Bakirov, Aslan - Yatsenko, Anzhelika

# Any questions about final project?

# Measuring uncertainty

## We can always measure something from our data...

...but how sure can we be about our measurement?

## We can always measure something from our data...

```{r, out.width='80%', fig.align='center'}
include_graphics("figures/sto_effect.png")
```

## But not necessarily significant!

```{r, out.width='80%', fig.align='center'}
include_graphics("figures/sto_effect_with_uncertainty.png")
```

## Why do have uncertainty in the measurement?

- If you knew the whole population, there wouldn't be uncertainty in your measurement
- But we only see 1 'segment' of the data = we have a sample of the population

## Sampling from a population

```{r sampling-from-height-1, out.width='80%', fig.align='center'}
sample_size <- 500
set.seed(1111)
dt <- data.table(sample_id = rep(1:1000, sample_size)) %>%
    .[, values_in_sample := rnorm(sample_size, mean = 178, sd = 7.5), sample_id] %>%
    .[, mean_of_sample := mean(values_in_sample), sample_id]

ggplot(dt[sample_id == 1], aes(values_in_sample)) +
    geom_histogram(aes(y = ..density..), fill = "darkgreen") +
    geom_vline(aes(xintercept = mean_of_sample)) +
    stat_function(
    	fun = dnorm,
    	args = list(mean = 178, sd = 7.5),
    	color = "red"
    ) +
    geom_vline(xintercept = 178, color = "red") +
    labs(
    	x = "Height (in cm)", y = NULL,
    	title = "Distribution of <span style = 'color: red;'>population</span> vs <span style = 'color: darkgreen;'>sample</span>"
    ) +
    theme_classic() +
    theme(text = element_text(size=20)) +
    theme(plot.title = element_markdown())
```

## Sampling from a population

```{r sampling-from-height-2, out.width='80%', fig.align='center'}
ggplot(dt[sample_id == 2], aes(values_in_sample)) +
    geom_histogram(aes(y = ..density..), fill = "darkgreen") +
    geom_vline(aes(xintercept = mean_of_sample)) +
    stat_function(
    	fun = dnorm,
    	args = list(mean = 178, sd = 7.5),
    	color = "red"
    ) +
    geom_vline(xintercept = 178, color = "red") +
    labs(
    	x = "Height (in cm)", y = NULL,
    	title = "Distribution of <span style = 'color: red;'>population</span> vs <span style = 'color: darkgreen;'>sample</span>"
    ) +
    theme_classic() +
    theme(text = element_text(size=20)) +
    theme(plot.title = element_markdown())
```

## Sampling from a population

```{r sampling-from-height, out.width='80%', fig.align='center', fig.show='animate', interval=1/20}
sampling_from_population_anim <- ggplot(dt[sample_id <= 100], aes(values_in_sample)) +
    geom_histogram(aes(y = ..density..), fill = "darkgreen") +
    geom_vline(aes(xintercept = mean_of_sample)) +
    stat_function(
    	fun = dnorm,
    	args = list(mean = 178, sd = 7.5),
    	color = "red"
    ) +
    geom_vline(xintercept = 178, color = "red") +
    labs(
    	x = "Height (in cm)", y = NULL,
    	title = "Distribution of <span style = 'color: red;'>population</span> vs <span style = 'color: darkgreen;'>sample</span>"
    ) +
    theme_classic() +
    theme(text = element_text(size=20)) +
    theme(plot.title = element_markdown()) +
    transition_states(sample_id)

sampling_from_population_anim
```

## Distribution of sample means

```{r distribution-of-sample-means, out.width='80%', fig.align='center'}
dt[, .(sample_id, mean_of_sample)] %>% unique() %>%
	ggplot(aes(mean_of_sample)) +
		geom_density(color = "darkgreen") +
        xlim(178 - 3*(7.5/sqrt(sample_size)), 178 + 3*(7.5/sqrt(sample_size))) +
        stat_function(
        	fun = dnorm,
        	args = list(mean = 178, sd = 7.5/sqrt(sample_size)),
        	color = "red"
        ) +
		geom_vline(
			xintercept = 178 - 1.96*(7.5/sqrt(sample_size)),
			color = "red"
		) +
		geom_vline(
			xintercept = 178 + 1.96*(7.5/sqrt(sample_size)),
			color = "red"
		) +
		geom_vline(
			aes(xintercept = quantile(mean_of_sample, 0.025))
		) +
		geom_vline(
			aes(xintercept = quantile(mean_of_sample, 0.975))
		) +
		geom_vline(xintercept = 178, color = "red") +
        labs(
	    	x = "Height (in cm)", y = NULL,
	    	title = "Distribution of <span style = 'color: darkgreen;'>sample means</span>
	    		<br> compared to <span style = 'color: red;'>normal distribution</span> with 'true' parameters
	    		<br> from population"
    	) +
        theme_classic() +
        theme(text = element_text(size=20)) +
        theme(plot.title = element_markdown())
```

## Distribution of sample means

Normal distribution with parameters:

$\bar x$ is the sample mean,

$s$ is the standard deviation of the sample distribution,

$n$ is the sample size,

add 95% 'CI' interval as:

$$\bar x \pm 1.96 * \frac{s}{\sqrt{n}}$$

## Law of Large Numbers

*The average of the results obtained from a large number of trials should be close to the expected value and will tend to become closer to the expected value as more trials are performed.*
 											- [Wikipedia](https://en.wikipedia.org/wiki/Law_of_large_numbers)

## Central Limit Theorem

*When independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed.*
 											- [Wikipedia](https://en.wikipedia.org/wiki/Central_limit_theorem)

<!-- ## What we do when we check for CI -->

<!-- ```{r calculating-ci-animation, out.width='80%', fig.align='center', fig.show='animate'} -->
<!-- sample_mean_w_CI <- dt[sample_id <= 100, .( -->
<!--     mean_of_sample = mean(values_in_sample), -->
<!--     SD_of_sample = sd(values_in_sample), -->
<!--     sample_size = .N), sample_id -->
<!-- ] %>% -->
<!--     .[, `:=`( -->
<!--         CI_lower = mean_of_sample - 1.96*(SD_of_sample/sqrt(sample_size)), -->
<!--         CI_higher = mean_of_sample + 1.96*(SD_of_sample/sqrt(sample_size)) -->
<!--     )] -->

<!-- sample_means_anim <- ggplot(sample_mean_w_CI, aes( -->
<!-- 	x = sample_id, y = mean_of_sample, -->
<!-- 	color = !or(CI_lower > 178, CI_higher < 178) -->
<!-- )) + -->
<!--     geom_point() + -->
<!--     geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher)) + -->
<!--     geom_hline(yintercept = 178, color = "black") + -->
<!--     coord_flip() + -->
<!--     labs(x = NULL, y = "Height (in cm)", title = "Mean and CI from different samples") + -->
<!--     theme_classic() + -->
<!--     theme(legend.position = "none") + -->
<!--     theme(text = element_text(size=20)) + -->
<!--     theme(plot.title = element_markdown()) + -->
<!--     transition_states(sample_id) -->

<!-- animate(sample_means_anim, width = 240, height = 240) %>% -->
<!--     anim_save( -->
<!--         filename = "figures/sample_means.gif", -->
<!--         duration = 5, fps = 20, -->
<!--         width = 240, height = 240 -->
<!--     ) -->
<!-- sample_means_mgif <- image_read("figures/sample_means.gif") -->

<!-- animate(sampling_from_population_anim, width = 240, height = 240) %>% -->
<!--     anim_save( -->
<!--         filename = "figures/sampling_from_population.gif", -->
<!--         duration = 5, fps = 20, -->
<!--         width = 240, height = 240 -->
<!--     ) -->
<!-- sampling_from_population_mgif <- image_read("figures/sampling_from_population.gif") -->

<!-- new_gif <- image_append(c(sampling_from_population_mgif[1], sample_means_mgif[1])) -->
<!-- for(i in 2:100){ -->
<!--   combined <- image_append(c(sampling_from_population_mgif[i], sample_means_mgif[i])) -->
<!--   new_gif <- c(new_gif, combined) -->
<!-- } -->

<!-- new_gif -->
<!-- ``` -->

## What are Confidence Intervals?

* The normal table gives us the fact that P[ -1.96 < Z < 1.96 ] = 0.95.

* With a sample of n values from a population with mean $\mu$ and standard deviation $\sigma$, the
Central Limit theorem gives us the result that Z = $\sqrt n\frac{\bar x - \mu}{\sigma}$ is approximately normally distributed with mean 0 and with standard deviation 1.

## What are Confidence Intervals?

```{r, out.width='75%', fig.align='center'}
include_graphics("figures/confidence_interval_proof.png")
```

<font size="2"> [http://people.stern.nyu.edu/gsimon/Pamphlets/ConfidenceIntervalCollection16APR08.pdf](http://people.stern.nyu.edu/gsimon/Pamphlets/ConfidenceIntervalCollection16APR08.pdf) </font>

## What are Confidence Intervals?

```{r calculating-ci, out.width='80%', fig.align='center'}
sample_mean_w_CI <- dt[sample_id <= 100, .(
    mean_of_sample = mean(values_in_sample),
    SD_of_sample = sd(values_in_sample),
    sample_size = .N), sample_id
] %>%
    .[, `:=`(
        CI_lower = mean_of_sample - 1.96*(SD_of_sample/sqrt(sample_size)),
        CI_higher = mean_of_sample + 1.96*(SD_of_sample/sqrt(sample_size))
    )]

ggplot(sample_mean_w_CI, aes(
	x = sample_id, y = mean_of_sample,
	color = !or(CI_lower > 178, CI_higher < 178)
)) +
    geom_point() +
    geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher)) +
    geom_hline(yintercept = 178, color = "black") +
    coord_flip() +
    labs(
    	x = NULL, y = "Height (in cm)",
    	title = "Mean and CI from different samples:
    	<br> About 95% of the CIs <span style = 'color: #00BFC4;'>contains</span> the true mean,
    	<br> but 5% <span style = 'color: #F8766D;'>does not contain</span> (just by chance)"
    ) +
    theme_classic() +
    theme(legend.position = "none") +
    theme(text = element_text(size=20)) +
    theme(plot.title = element_markdown())
```

## Distribution of sample means with different distributions

```{r coin-flipping-sample-one, out.width='80%', fig.align='center'}
set.seed(1234)
sample_size <- 100
coin_flip_dt <- data.table(sample_id = rep(1:100, sample_size)) %>%
    .[, .(coin_flip = rbinom(100, 1, 0.5)), sample_id] %>%
    .[, coin_flip := ifelse(coin_flip == 1, "heads", "tails")]

ggplot(coin_flip_dt[sample_id == 1], aes(coin_flip, fill = coin_flip)) +
    geom_histogram(stat = "count") +
    labs(x = NULL) +
    theme_classic() +
    theme(text = element_text(size=20)) +
    theme(legend.position = "none")
```

## Distribution of sample means with different distributions

```{r coin-flipping-sample-two, out.width='80%', fig.align='center'}
set.seed(1234)
sample_size <- 100
coin_flip_dt <- data.table(sample_id = rep(1:100, sample_size)) %>%
    .[, .(coin_flip = rbinom(100, 1, 0.5)), sample_id] %>%
    .[, coin_flip := ifelse(coin_flip == 1, "heads", "tails")]

ggplot(coin_flip_dt[sample_id == 2], aes(coin_flip, fill = coin_flip)) +
    geom_histogram(stat = "count") +
    labs(x = NULL) +
    theme_classic() +
    theme(text = element_text(size=20)) +
    theme(legend.position = "none")
```

## Distribution of sample means with different distributions

```{r coin-flipping-simulation, out.width='80%', fig.align='center', fig.show='animate', interval=1/20}
set.seed(123)
sample_size <- 100
coin_flip_dt_small_sample <- data.table(sample_id = rep(1:100, sample_size)) %>%
    .[, .(coin_flip = rbinom(sample_size, 1, 0.5)), sample_id] %>%
    .[, coin_flip := ifelse(coin_flip == 1, "heads", "tails")]

ggplot(coin_flip_dt_small_sample, aes(coin_flip, fill = coin_flip)) +
    geom_histogram(stat = "count") +
    labs(x = NULL) +
    theme_classic() +
    theme(text = element_text(size=20)) +
    theme(legend.position = "none") +
    transition_states(sample_id)
```

## Distribution of sample means with different distributions

```{r coin-flipping-small-sample-means, out.width='80%', fig.align='center'}
coin_flip_dt_small_sample %>%
    .[, .(num_head = mean(coin_flip == "heads")), sample_id] %>%
    ggplot(aes(num_head), color = "darkgreen") +
    geom_density() +
    xlim(0.3, 0.7) +
    labs(x = "ratio of heads") +
    theme_classic() +
    theme(text = element_text(size=20))
```

## Why does sample size matter?

```{r coin-flipping-large-sample-means, out.width='80%', fig.align='center'}
set.seed(123)
sample_size <- 1000
coin_flip_dt_big_sample <- data.table(sample_id = rep(1:100, sample_size)) %>%
    .[, .(coin_flip = rbinom(sample_size, 1, 0.5)), sample_id] %>%
    .[, coin_flip := ifelse(coin_flip == 1, "heads", "tails")]

coin_flip_dt_big_sample %>%
    .[, .(num_head = mean(coin_flip == "heads")), sample_id] %>%
    ggplot(aes(num_head), color = "darkgreen") +
    geom_density() +
    xlim(0.3, 0.7) +
    labs(x = "ratio of heads") +
    theme_classic() +
    theme(text = element_text(size=20))
```

## What are the key assumptions?

- we have a random sample from the distribution (= independent and identically distributed random variables are drawn)
- finite variance / distribution is not 'long tailed'

## What if variance is infinite?

```{r sales-example}
# download from https://drive.google.com/drive/folders/1HmCp1pTpEaqta7z0p1fnCas1dN6myVxg
sales <- fread("sales_sample.csv")
ggplot(sales, aes(sales_amount)) +
	geom_density() +
	theme_classic() +
	theme(text = element_text(size=20))
# sales[, mean(sales_amount)]
```

## It stayes very skewed even if we zoom in

```{r sales-example-zoom}
ggplot(sales[sales_amount <= 1000], aes(sales_amount)) +
    geom_density() +
    theme_classic() +
    theme(text = element_text(size=20))
```

## What if variance is infinite?

```{r estimate-sales-distribution}
set.seed(1478)
# mean(rlnorm(10000, 4, 1.7))
ggplot(mapping = aes(rlnorm(10000, 4, 1.7))) +
	geom_density() +
	theme_classic() +
	theme(text = element_text(size=20))
```

## Distribution of avg. sales amount from samples

```{r sample-from-sales}
set.seed(1478)
sample_size <- 10000
sales_sample <- data.table(sample_id = rep(1:100, sample_size)) %>%
    .[, .(sales_amount = rlnorm(sample_size, 4, 1.7)), sample_id]

sales_sample %>%
    .[, .(avg_sales_amount = mean(sales_amount)), sample_id] %>%
    ggplot(aes(avg_sales_amount)) +
    geom_density() +
    xlim(200, 300) +
    labs(x = "Avg. sales amount") +
    theme_classic() +
    theme(text = element_text(size=20))
```

## How can we calculate the uncertainty of our measurement?

- **Based on variance of known distribution**
- Monte-Carlo method
- Bootstrapping
- (and other methods as well of course)

## Calculate uncertainty based on variance

Calculate interval to show how wide range we need in order to be 'sure' that we have the real value included.

```{r normal-distribution-w-2sigma, out.width='75%', fig.align='center'}
ggplot() +
	stat_function(fun = dnorm) +
	geom_vline(xintercept = -2) +
	geom_vline(xintercept = 2) +
    scale_x_continuous(
    	limits = c(-3, 3),
    	breaks = seq(-3, 3, 1)
	) +
    labs(x = NULL, y = NULL) +
    theme_classic()
```

## How to calculate uncertainty from sampling distribution

By CLT + LLN, you can add uncertainty to your point estimate (for a 95% Confidence Interval), such as:

$$\bar x \pm 1.96 * \frac{\sigma}{\sqrt{n}}$$

$\bar x$ is the sample mean,

$\sigma$ standard deviation of the population,

$n$ is the sample size

## Standard Error

We want to calculate the variance of means of different samples.

Since the standard devation for the population is rarely known, we estimate SE as:

$Standard\,Error = \frac{s}{\sqrt{n}}$

$s$ standard deviation of the sample,

$n$ is the sample size

## Standard Error vs Standard Deviation

Looks similar, but it's not the same!

Derivation for Standard Error:

```{r, out.width='70%', fig.align='center'}
include_graphics("figures/standard_error_derivation.png")
```

[Derivation is from here.](https://stats.stackexchange.com/questions/60484/why-is-the-formula-for-standard-error-the-way-it-is)

## Calculating uncertainty for proportion

Distribution of sample means of Bernoulli distr. is normally distributed with parameters:

$$ \bar x = p $$

$$ \sigma = \sqrt{\frac{p*(1-p)}{n}} $$

## Calculating uncertainty for absolute uplift

Distribution of difference of sample means is also normally distributed with parameters:

$$ \bar x_{diff} = \bar x_{1} - \bar x_{2} $$

$$ \sigma_{diff} = \sqrt{\frac{\sigma_{1}^{2}}{n_{1}} + \frac{\sigma_{2}^{2}}{n_{2}}} $$

## Calculating uncertainty

We can use the same formula:

$$\bar x \pm 1.96 * \frac{\sigma}{\sqrt{n}}$$

Just use the right equations for $\bar x$ and $\sigma$, depending on the distribution!

## Calculating uncertainty for relative uplift

Distribution of ratio of sample means is ??

How to calculate it? -> Next time :)

## Calculate uncertainty for a point estimate

```{r, out.width='60%'}
dt <- fread("experiment_result_2.csv")
head(dt) %>% kable()
```

## How to calculate the open rate absolute uplift with uncertainty!

Let me show that!
```{r calculate-uplift-with-uncertainty}
# dt[, .(open_rate = sum(num_open) / sum(num_send), num_send = sum(num_send)), group] %>%
#     dcast(. ~ group, value.var = c("open_rate", "num_send")) %>%
#     .[, .(
#         mean = open_rate_treatment - open_rate_control,
#         se = sqrt(
#             (open_rate_treatment * (1-open_rate_treatment) / num_send_treatment) +
#             (open_rate_control * (1-open_rate_control) / num_send_control)
#         )
#     )] %>%
#     .[, .(
#         mean,
#         CI_lower = mean - 1.96 * se,
#         CI_lower = mean + 1.96 * se
#     )]
```

## Now your turn!
### Calculate the open rate and the uncertainty for both groups!
```{r calculate-OR-with-uncertainty}

```

## Results from an experiment - How to plot uncertainty?

```{r}
open_rate_dt <- dt[,
    .(open_rate = sum(num_open) / sum(num_send),
      num_send = sum(num_send)),
    by = group
]

ggplot(open_rate_dt, aes(x = group, y = open_rate, fill = group)) +
    geom_col() +
    theme_light() +
    labs(x = NULL, y = "open rate") +
    scale_y_continuous(labels = scales::percent)
```

## How to plot uncertainty

```{r}
open_rate_dt %>%
    copy() %>%
    .[, CI_lower := open_rate - (1.96 * sqrt(open_rate * (1 - open_rate) / num_send))] %>%
    .[, CI_higher := open_rate + (1.96 * sqrt(open_rate * (1 - open_rate) / num_send))] %>%
    ggplot(aes(x = group, y = open_rate, fill = group)) +
        geom_col() +
        geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher)) +
        theme_light() +
        labs(x = NULL, y = "open rate") +
        scale_y_continuous(labels = scales::percent)
```

## How to plot uncertainty - some examples
```{r, out.width='80%', fig.align='center'}
include_graphics("figures/uplift.png")
```

## How to plot uncertainty - some examples
```{r, out.width='80%', fig.align='center'}
include_graphics("figures/uplift_for_customer.png")
```

## How to plot uncertainty - some examples
```{r, out.width='80%', fig.align='center'}
include_graphics("figures/sto_effect_with_uncertainty.png")
```

## Now your turn!
### 1. Calculate the click rate and the uncertainty!

```{r}
# click_rate_dt <- dt[, .(
#     click_rate = sum(num_click) / sum(num_send),
#     num_send = sum(num_send)
#     ), by = group
# ] %>%
#     .[, CI_lower := click_rate - (1.96 * sqrt(click_rate * (1 - click_rate) / num_send]))] %>%
#     .[, CI_higher := click_rate + (1.96 * sqrt(click_rate * (1 - click_rate) / num_send]))]
```

### 2. Plot the results! What do you see on the plots? Are the results significant?

```{r}
# ggplot(click_rate_dt, aes(
#     x = group, y = click_rate, fill = group
# )) +
#     geom_col() +
#     geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher)) +
#     theme_light() +
#     labs(x = NULL, y = "open rate") +
#     scale_y_continuous(labels = scales::percent)
```

## Bayesian uncertainty

Confidence Interval: If we would resample from our population, 95% of times the Confidence Interval will contain the true, unknown parameter.

Credible Interval: There is a 95% chance that this interval contains the true parameter.

## Difference in calculating uncertainty

Confidence Interval: Based on sampling distribution of means

Credible Interval: Based on data and prior belief

```{r, out.height='70%', fig.align='center'}
include_graphics("figures/bayes_posterior_from_prior_and_data.jpg")
```

## 95% Credible interval
```{r credible-interval, out.width='80%', fig.align='center'}
set.seed(1)
sample_size <- 100
coin_flip_dt_bayes <- data.table(sample_id = rep(1:100, sample_size)) %>%
    .[, .(coin_flip = rbinom(sample_size, 1, 0.5)), sample_id] %>%
    .[, coin_flip := ifelse(coin_flip == 1, "heads", "tails")] %>%
    .[, .(num_head = sum(coin_flip == "heads")), sample_id] %>%
    .[, `:=`(
        head_ratio = num_head / sample_size,
        CI_lower = qbeta(0.025, 1 + num_head, 1 + sample_size - num_head),
        CI_higher = qbeta(0.975, 1 + num_head, 1 + sample_size - num_head)
    ), sample_id]

ggplot(coin_flip_dt_bayes, aes(
    sample_id, head_ratio, color = !or(CI_lower > 0.5, CI_higher < 0.5)
)) +
    geom_point() +
    geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher)) +
    geom_hline(yintercept = 0.5, color = "black") +
    coord_flip() +
    labs(x = NULL, y = "ratio of heads") +
    theme_classic() +
    theme(legend.position = "none")

```

## Confidence Interval vs Credible Interval

- Credible Interval is easier to understand
- Credible Interval gives smaller interval if we have some prior knowledge
    - eg.: Use group averages as prior for contact level data
- With wrong prior provided, our posterior distribution is going to be wrong as well!

- With a lot of data or with non-informative priors, the two intervals are about the same

```{r conf-int-vs-cred-int, out.height='70%', fig.align='center'}
set.seed(1)
sample_size <- 100
coin_flip_dt <- data.table(coin_flip = rbinom(sample_size, 1, 0.5)) %>%
    .[, coin_flip := ifelse(coin_flip == 1, "heads", "tails")] %>%
    .[, .(
    	num_head = sum(coin_flip == "heads"),
    	head_ratio = sum(coin_flip == "heads") / sample_size
    )] %>%
    .[, `:=`(
        cred_int_lower = qbeta(0.05, 50 + num_head, 50 + sample_size - num_head),
        cred_int_higher = qbeta(0.95, 50 + num_head, 50 + sample_size - num_head),
        conf_int_lower = head_ratio - (1.96 * sqrt((head_ratio * (1 - head_ratio)) / sample_size)),
        conf_int_higher = head_ratio + (1.96 * sqrt((head_ratio * (1 - head_ratio)) / sample_size))
    )]

kable(coin_flip_dt[, .(head_ratio, cred_int_lower, cred_int_higher)])
kable(coin_flip_dt[, .(head_ratio, conf_int_lower, conf_int_higher)])
```

## Calculate uncertainty over time

```{r uncertainty-over-time, out.height='70%', fig.align='center'}
dt <- fread("experiment_results_over_time.csv")

uncertainty_over_time_dt <- dt[, .(open_rate = sum(did_open) / .N, num_send = .N), .(date, group)] %>%
    dcast(date ~ group, value.var = c("open_rate", "num_send")) %>%
    .[, `:=`(
        absolute_uplift = open_rate_treatment - open_rate_control,
        standard_error = sqrt(open_rate_treatment * (1 - open_rate_treatment) / num_send_treatment + open_rate_control * (1 - open_rate_control) / num_send_control)
    )] %>%
    .[, `:=`(
        CI_lower = absolute_uplift - 1.96 * standard_error,
        CI_higher = absolute_uplift + 1.96 * standard_error
    )]

ggplot(uncertainty_over_time_dt, aes(date, absolute_uplift)) +
    geom_point() +
    geom_line() +
    geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher)) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(x = "Date", y = "Absolute uplift (%point)") +
    theme_classic()
```
## Your turn!

Try to re-create the plot seen before, using `experiment_results_over_time.csv`!

## Takeaways

- Always show uncertainty
- Think about your audience

# Homework for next week and presenters

## Homework for next week

1. Figure out your research question / hypothesis!

2. Show (plot) that if there is a difference between two/multiple groups, add uncertainty as well!
    a. If it makes sense for your research question, do it for that! Eg.: Is there a difference in house prices on Buda compared to Pest?
    b. If not, use `experiment_result_HW.csv`: calculate the the point estimates and add uncertainty to one of your KPIs (defined by you in last week's homework) for both periods! Include the *absolute* uplift in the subtitle with confidence intervals!

## Presenting next week
Both Márton - Kamenár Gyöngyvér

Emerson, Ian - Ralbovszki Judit

Bat-Erdene, Boldmaa - Kashirin, Andrey
