---
title: "Eltecon Data Science Course by Emarsys"
subtitle: "Measuring uncertainty"
author: "András Bérczi"
date: "October 14, 2020"
output:
  beamer_presentation:
    colortheme: dolphin
    fonttheme: structurebold
    theme: AnnArbor
    # toc: true
    slide_level: 2
header-includes:
   - \usepackage{animate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
# knitr::opts_knit$set(root.dir='..')

library(knitr)
library(data.table)
library(magrittr)
library(ggplot2)
library(gganimate)
```

# Homeworks from last week

Nguyen Thai Duong - Szentistványi János

Kovács Ádám - Nguyen Nam Son

Bakirov, Aslan - Yatsenko, Anzhelika

# Any questions about final project?

# Measuring uncertainty

## We can always measure something from our data...

...but how sure can we be about our measurement?

## We can always measure something from our data...

```{r, out.width='80%', fig.align='center'}
include_graphics("figures/sto_effect.png")
```

## But not necessarily significant!

```{r, out.width='80%', fig.align='center'}
include_graphics("figures/sto_effect_with_uncertainty.png")
```

## Why do have uncertainty in the measurement?

- If you knew the whole population, there wouldn't be uncertainty in your measurement
- But we only see 1 'segment' of the data = we have a sample of the population

## Sampling from a population

```{r sampling-from-height, out.width='80%', fig.align='center', fig.show='animate', interval=1/20}
sample_size <- 1000
set.seed(123)
dt <- data.table(group = rep(1:100, sample_size)) %>%
    .[, values_in_sample := rnorm(sample_size, mean = 178, sd = 7.5), group] %>%
    .[, mean_of_sample := mean(values_in_sample), group]

ggplot(dt, aes(values_in_sample)) +
    geom_histogram(aes(y = ..density..)) +
    geom_vline(aes(xintercept = mean_of_sample)) +
    stat_function(
    	fun = dnorm,
    	args = list(mean = 178, sd = 7.5),
    	color = "red"
    ) +
    geom_vline(xintercept = 178, color = "red") +
    transition_states(group)
```

## Distribution of sample means - LLN + CLT

```{r distribution-of-sample-means, out.width='80%', fig.align='center'}
dt[, .(group, mean_of_sample)] %>% unique() %>%
	ggplot(aes(mean_of_sample)) +
		geom_density() +
        xlim(178 - 3*(7.5/sqrt(sample_size)), 178 + 3*(7.5/sqrt(sample_size))) +
        stat_function(
        	fun = dnorm,
        	args = list(mean = 178, sd = 7.5/sqrt(sample_size)),
        	color = "red"
        ) +
		geom_vline(
			xintercept = 178 - 1.96*(7.5/sqrt(sample_size)),
			color = "red"
		) +
		geom_vline(
			xintercept = 178 + 1.96*(7.5/sqrt(sample_size)),
			color = "red"
		) +
		geom_vline(xintercept = 178, color = "red")
```

## What we do when we check for CI

```{r calculating-ci, out.width='80%', fig.align='center'}
sample_mean_w_CI <- dt[, .(
    mean_of_sample = mean(values_in_sample),
    SD_of_sample = sd(values_in_sample),
    sample_size = .N), group
] %>%
    .[, `:=`(
        CI_lower = mean_of_sample - 1.96*(SD_of_sample/sqrt(sample_size)),
        CI_higher = mean_of_sample + 1.96*(SD_of_sample/sqrt(sample_size))
    )]
ggplot(sample_mean_w_CI, aes(
	x = group, y = mean_of_sample,
	color = any(CI_lower > 178, CI_higher < 178)
)) +
    geom_point() +
    geom_errorbar(aes(ymin = CI_lower, ymax = CI_higher)) +
    geom_hline(yintercept = 178, color = "red") +
    coord_flip()
```

## What are the key assumptions?

- i.i.d. sampling
- finite variance

## How can we calculate uncertainty to our measurement?

- Based on variance of known distribution
- Monte-Carlo method
- Bootstrapping
- (and other methods as well of course)

