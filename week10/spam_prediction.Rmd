---
title: "Spam classification case study"
author: "Tamas Koncz"
date: "11/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.height = 4.5, fig.width = 9)
knitr::opts_chunk$set(fig.align='center')

library(knitr)
library(data.table)
library(ggplot2)
library(magrittr)
library(tidytext)
library(glue)
library(here)
library(patchwork)
library(rpart)

source(here::here("week10", "eval_utils.R"))
```

## Exploratory analysis

We are going to work on the [sms-spam-or-ham](https://www.kaggle.com/dejavu23/sms-spam-or-ham-beginner/data) dataset from Kaggle:

_"The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text."_

I recommend to take a look at the raw `.csv` first. Then, we can import it properly to R:

```{r}
sms <- fread(
    here::here("week10", "data", "spam.csv"),
    header = TRUE,
    select = c("v1", "v2"),
    col.names = c("is_spam", "sms_text"),
    encoding = 'Latin-1'
) %>%
    .[, is_spam := ifelse(is_spam == "spam", 1, 0)]
```

Let's see the frequency of spam messages in the dataset:

```{r}
sms %>%
    .[, .N, by = "is_spam"] %>%
    .[, share := scales::percent(N / sum(N), accuracy = 0.01)] %>%
    .[]
```

Our dataset is quite imbalanced. We'll have to pay attention to this when modeling.

### Message length

```{r}
sms[, sms_chr_length := nchar(sms_text)]
```

```{r}
sms[, .(avg_sms_chr_length = mean(sms_chr_length)), by = "is_spam"]
```

Okay, this is interesting... let's dive deeper:

```{r, echo = FALSE}
spam_chr_length     <- sms[is_spam == 1, .(median_sms_chr_length = median(sms_chr_length))]
non_spam_chr_length <- sms[is_spam == 0, .(median_sms_chr_length = median(sms_chr_length))]

ggplot(
    sms,
    aes(x = sms_chr_length, fill = as.factor(is_spam), color = as.factor(is_spam))
) +
    geom_density(alpha = 0.5) +
    labs(
        title = "Densities number of characters by sms type",
        subtitle = glue(
            "Median sms char. lengths: spam = {spam_chr_length}",
            ", non-spam: {non_spam_chr_length}"
        )
    ) +
    theme_minimal() +
    theme(legend.position = "top")
```

We might have just found our baseline model!

### Tokenization


Add a unique id to each message:

```{r}
sms[, message_id := 1:.N]
```

Tokenization:

```{r}
sms_words <- sms %>%
    .[, .(message_id, sms_text)] %>%
    tidytext::unnest_tokens(output = "word", input = "sms_text") %>%
    data.table()
```

**EXERCISE** let's flag stopwords:
```{r}
stop_words <- tidytext::get_stopwords() %>%
    data.table() %>%
    .[lexicon == "snowball", -c("lexicon")] %>%
    .[, is_stopword := TRUE]

sms_words <- sms_words %>%
    merge(stop_words, by = "word", all.x = TRUE) %>%
    .[, is_stopword := dplyr::coalesce(is_stopword, FALSE)]
```

Let's flag other "stopwords" manually: only digits and single letters.
```{r}
sms_words %>%
    .[, is_stopword := ifelse(nchar(word) == 1, TRUE, is_stopword)] %>%
    .[, is_stopword := ifelse(grepl("^\\d+$", word), TRUE, is_stopword)]
```

#### Number of words

First, let's see sms length by words!

```{r}
sms_word_lengths <- merge(
    sms_words[, .(num_words = .N), by = "message_id"],
    sms_words[is_stopword == FALSE, .(num_non_stopwords = .N), by = "message_id"],
    by = "message_id",
    all.x = TRUE
) %>%
    .[, num_non_stopwords := dplyr::coalesce(num_non_stopwords, 0)]
```

```{r}
sms <- merge(
    sms, sms_word_lengths, by = "message_id"
)
```

**EXERCISE** plot the distribution of `num_words` by `is_spam`!
```{r, echo = FALSE}
spam_num_words     <- sms[is_spam == 1, .(median_num_words = median(num_words))]
non_spam_num_words <- sms[is_spam == 0, .(median_num_words = median(num_words))]

ggplot(
    sms,
    aes(x = num_words, fill = as.factor(is_spam), color = as.factor(is_spam))
) +
    geom_histogram(alpha = 0.25, position = "identity") +
    labs(
        title = "Distribution of number of words by sms type",
        subtitle = glue(
            "Median sms word count: spam = {spam_num_words}",
            ", non-spam: {non_spam_num_words}"
        )
    ) +
    theme_minimal() +
    theme(legend.position = "top")
```

```{r, echo = FALSE}
spam_num_non_stopwords     <- sms[
    is_spam == 1, .(median_num_non_stopwords = median(num_non_stopwords))
]
non_spam_num_non_stopwords <- sms[
    is_spam == 0, .(median_num_non_stopwords = median(num_non_stopwords))
]

ggplot(
    sms,
    aes(x = num_non_stopwords, fill = as.factor(is_spam), color = as.factor(is_spam))
) +
    geom_histogram(alpha = 0.25, position = "identity") +
    labs(
        title = "Distribution of number of non-stopwords by sms type",
        subtitle = glue(
            "Median sms word count: spam = {spam_num_non_stopwords}",
            ", non-spam: {non_spam_num_non_stopwords}"
        )
    ) +
    theme_minimal() +
    theme(legend.position = "top")
```

## Most frequent words

```{r}
sms_words <- merge(
    sms_words, sms[, .(message_id, is_spam)], by = "message_id"
)
```

```{r}
num_spam_msgs <- sms_words[is_spam == 1, uniqueN(message_id)]
word_occurrences_in_spam <- sms_words[
    is_spam == 1,
    .(prevalence_in_spam = uniqueN(message_id) / num_spam_msgs),
    by = c("word", "is_stopword")
]

num_non_spam_msgs <- sms_words[is_spam == 0, uniqueN(message_id)]
word_occurrences_in_non_spam <- sms_words[
    is_spam == 0,
    .(prevalence_in_non_spam = uniqueN(message_id) / num_non_spam_msgs),
    by = c("word", "is_stopword")
]

word_occurrences <- merge(
    word_occurrences_in_spam, word_occurrences_in_non_spam,
    by = c("word", "is_stopword"), all = TRUE
) %>%
    .[, `:=`(
        prevalence_in_spam = dplyr::coalesce(prevalence_in_spam, 0),
        prevalence_in_non_spam = dplyr::coalesce(prevalence_in_non_spam, 0)
    )] %>%
    .[, avg_prevalence := (prevalence_in_spam + prevalence_in_non_spam) / 2]
```

```{r}
p1 <- word_occurrences[order(-avg_prevalence)][1:20] %>%
    ggplot(aes(x = reorder(word, avg_prevalence))) +
        geom_col(aes(y = prevalence_in_spam), fill = "red") +
        labs(x = "avg_prevalence", title = "Spam") +
        ylim(c(0, 0.65)) +
        coord_flip() +
        theme_minimal() +
        theme(legend.position = "top")

p2 <- word_occurrences[order(-avg_prevalence)][1:20] %>%
    ggplot(aes(x = reorder(word, avg_prevalence))) +
        geom_col(aes(y = prevalence_in_non_spam), fill = "blue") +
        labs(x = "avg_prevalence", title = "Non-spam") +
        ylim(c(0, 0.65)) +
        coord_flip() +
        theme_minimal() +
        theme(legend.position = "top")

patchwork::wrap_plots(p1, p2) +
    patchwork::plot_annotation(
          title = "Prevalence of most frequent words",
          subtitle = "Includes stopwords",
    )
```

```{r}
p1 <- word_occurrences[is_stopword == FALSE][order(-avg_prevalence)][1:20] %>%
    ggplot(aes(x = reorder(word, avg_prevalence))) +
        geom_col(aes(y = prevalence_in_spam), fill = "red") +
        labs(x = "avg_prevalence", title = "Spam") +
        ylim(c(0, 0.65)) +
        coord_flip() +
        theme_minimal() +
        theme(legend.position = "top")

p2 <- word_occurrences[is_stopword == FALSE][order(-avg_prevalence)][1:20] %>%
    ggplot(aes(x = reorder(word, avg_prevalence))) +
        geom_col(aes(y = prevalence_in_non_spam), fill = "blue") +
        labs(x = "avg_prevalence", title = "Non-spam") +
        ylim(c(0, 0.65)) +
        coord_flip() +
        theme_minimal() +
        theme(legend.position = "top")

patchwork::wrap_plots(p1, p2) +
    patchwork::plot_annotation(
          title = "Prevalence of most frequent words",
          subtitle = "Excludes stopwords",
    )
```

## Predictive modeling


### Train / test split

First, we have to separate a portion of our dataset for testing the models we create:
```{r}
train_proportion <- 0.8

num_rows <- sms[, .N]

set.seed(20211015)
train_index <- sample(1:num_rows, floor(num_rows * train_proportion))

train <- sms[train_index]
test  <- sms[-train_index]
```

### Baseline model

Most ML project should start by establishing a useful baseline.
Automatically assigning the most frequent class could be a good performance benchmark, but it is rarely useful in practice.

Rather, we will build a single-split tree model to form our baseline:

```{r}
baseline_model <- rpart(
    is_spam ~ sms_chr_length,
    data = train,
    control = rpart.control(maxdepth = 1)
)
baseline_model
```

### Evaluation of baseline model

```{r}
baseline_predictions <- predict(baseline_model, newdata = test)
```

Predictions will only take on two values (due to the single-split control), so we can take their avg. as our cutoff to assign classes to predicted probabilities:
```{r}
baseline_cutoff <- mean(unique(baseline_predictions))
```

Accuracy (remember, this is a imbalanced dataset):

```{r}
calculateAccuracy(test[["is_spam"]], baseline_predictions, cutoff = baseline_cutoff)
```

Confusion matrix to visualize the TPR / FPR balance:

```{r}
baseline_confusion_matrix <- getConfusionMatrix(
    actual = test[["is_spam"]],
    predictions = baseline_predictions,
    cutoff = baseline_cutoff
)
```

```{r}
calculateFPRTPR(baseline_confusion_matrix)
```

We are trading in a lot of false positives for our true positives.
Hopefully, we can do better with our next models.

### Logit using all features

```{r}
logit_model <- glm(
    is_spam ~ .,
    family = binomial(link = "logit"),
    data = train[, -c("message_id", "sms_text")]
)

summary(logit_model)
```

```{r}
logit_predictions <- predict(logit_model, newdata = test, type = "response")
```

```{r}
logit_fpr_tpr <- purrr::map(seq(0, 1, 0.05), ~{
    calculateFPRTPR(getConfusionMatrix(
        actual = test[["is_spam"]],
        predictions = logit_predictions,
        cutoff = .x
    )) %>%
        .[, p := .x]
}) %>%
    rbindlist()
```

```{r}
plotRoc(logit_fpr_tpr, model_name = "full logit model")
```

```{r}
logit_confusion_matrix <- getConfusionMatrix(
    actual = test[["is_spam"]], predictions = logit_predictions, cutoff = 0.2
)
```

```{r}
calculateFPRTPR(logit_confusion_matrix)
```
