---
title: Unsupervised Learning
subtitle: Eltecon Data Science Course by Emarsys
author: János Divényi
date: December 2, 2020
output:
    beamer_presentation:
        colortheme: dolphin
        fonttheme: structurebold
        theme: AnnArbor
        # toc: true
        slide_level: 2
header-includes:
   - \usepackage{animate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
# knitr::opts_knit$set(root.dir = '..')

library(data.table)
library(ggplot2)
library(magrittr)
```

# Supervised vs unsupervised learning
## What is the difference between Supervised and Unsupervised learning?
```{r, out.width = "100%", fig.align = "center", echo=FALSE}
knitr::include_graphics("figures/supervised_vs_unsupervised.png")
```

## Supervised learning
- For each observation of the predictor measurements (**X**) there is an associated response measurement (*Y*).

- The goal is to fit a model that predicts the amount or the label of the response.

- E.g. linear regression, logistic regression, classification etc.

## Unsupervised learning
- We observe measurements (**X**), but no associated response variable (*Y*), so we cannot fit any regressions.

- The goal is to find relationship or structure among the measurements.

## Goals of unsupervised learning
- Find patterns in the features of the data by dimensionality reduction
    - Ex 1. instead of using both humidity and rainfall in a classification problem, they can be collapsed into just one underlying feature, since both of them are strongly correlated
\linebreak
- Find homogenous subgroups (clusters) within a population
    - Ex 1. segmenting consumers based on demographics and purchasing history
    - Ex 2. find similar movies based on features of each movie and reviews of the movies

# Dimensionality reduction with PCA

## Dimensionality reduction with PCA
- *Principal components* allow us to summarize a large set of correlated variables with a smaller number of representative variables that collectively explain most of the variablility in the original set.

- *Principal component analysis* (PCA) is simply reducing the number of variables of a data set, while preserving as much information as possible.

- Reducing the number of variables comes at the expense of accuracy, so with PCA we trade a little accuracy for simplicity.

## What are principal components?
- Try to visualize *n* observations with measurements of *p* features by two-dimensional scatterplots (with *p* = 10 there are 45 plots!).

- Instead we'd like to find a low-dimensional representation of the data that captures most of the information.

- Imagine that each of the *n* observations lives in a *p*-dimensional space, but not all of these dimensions are equally *interesting*.

- Interesting is measured by the amount that the observations vary along each dimension.

- Each of the dimensions (principal components) found by PCA is a linear combination of the *p* features.


## PCA in R
```{r}
USArrests <- as.data.table(USArrests)
head(USArrests)
```

## PCA in R
```{r}
USArrests[, lapply(.SD, mean)]
```

```{r}
USArrests[, lapply(.SD, var)]
```

## PCA in R
```{r}
pca_output <- prcomp(USArrests, scale = TRUE)
```
```{r}
pca_output$center
pca_output$scale
```

- ``center`` and ``scale`` are the *means* and *standard deviations* of the variables that were used for scaling prior to implementing PCA

## PCA in R
```{r}
pca_output$rotation
```

- ``rotation`` is the matrix whose columns contain the weights (loadings) for the linear feature combinations of the principal components (mathematically, they are the eigenvectors)

## PCA in R
```{r}
dim(pca_output$x)
head(pca_output$x)
```
- matrix ``x`` has as its columns the principal component score vectors


## PCA in R
```{r}
pca_var <- pca_output$sdev^2
pca_var
```

```{r}
pca_var_explained <- pca_var / sum(pca_var)
pca_var_explained
```

## PCA in R
```{r, out.width = "80%", fig.align = "center"}
plot(pca_var_explained, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
```

## PCA in R
```{r, out.width = "80%", fig.align = "center"}
plot(cumsum(pca_var_explained), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
```


# K-means clustering

## K-means theory
- Clustering the observations of a data set means partitioning them into groups so that observations within each group are quite *similar*, while observations in different groups are quite *different* from each other.

- Each observation should belong to exactly one cluster.

- To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters.

## K-means clustering with different values of K
```{r, out.width = "100%", fig.align = "center", echo=FALSE}
knitr::include_graphics("figures/kmeans_different_ks.png")
```

## K-means optimization problem
- A *good* clustering is one for which the *within-cluster variation* is as small as possible.
- If the within-cluster variation for cluster *C~k~* is a measure *W(C~k~)*, then we want to solve:
$$\underset{C_{1},...,C_{K}}{\text{minimize}}\left\{\sum_{k=1}^{K}W(C_{k})\right\}$$
- The most common choice of measure is the *squared Euclidean distance*:
$$W(C_{k})=\frac{1}{|C_{k}|}\sum_{i,i' \in C_{k}}\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2$$
- Thus the optimization problem is:
$$\underset{C_{1},...,C_{K}}{\text{minimize}}\left\{\sum_{k=1}^{K}\frac{1}{|C_{k}|}\sum_{i,i' \in C_{k}}\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2\right\}$$

## K-means algorithm
1. Randomly assign a number, from 1 to *K*, to each of the observations. These serve as initial cluster assignments for the observations.

2. Iterate until the cluster assignments stop changing:
    (a) For each of the *K* clusters, compute the cluster *centroid*. The *k*th cluster centroid is the vector of the *p* feature means for the observations in the *k*th cluster.
    (b) Assign each observation to the cluster whose centroid is closest (where *closest* is defined using Euclidean distance).

## K-means algorithm (cont.)
- The results will depend on the initial (random) cluster assignment in Step 1, thus we need to run the algorithm multiple times from different random initial
configurations.

- We select the *best* solution, for which the objective is the smallest.

## K-means algorithm explained
- Notice that:
$$\frac{1}{|C_{k}|}\sum_{i,i'\in C_{k}}\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2=2\sum_{i\in C_{k}}\sum_{j=1}^{p}(x_{ij}-\bar{x}_{kj})^2$$
where $$\bar{x}_{kj}=\frac{1}{|C_{k}|}\sum_{i\in C_{k}}x_{ij}$$ is the mean for feature *j* in cluster *C~k~*

- The above shows that in Step 2(a) the cluster means for each feature are the constants that minimize the sum-of-squared deviations, and by reallocation in Step 2(b) we can only improve.

- When the result no longer changes, a *local optimum* has been reached.

## K-means clustering progression
```{r, out.width = "100%", fig.align = "center", echo=FALSE}
knitr::include_graphics("figures/kmeans_iterations_1.png")
```

## K-means clustering progression (cont.)
```{r, out.width = "100%", fig.align = "center", echo=FALSE}
knitr::include_graphics("figures/kmeans_iterations_2.png")
```

## K-means clustering with K=2
```{r}
set.seed(2)
x <- data.table(a = rnorm(50), b = rnorm(50))
x[1:25, `:=`(a = a + 2, b = b - 3)]
```

```{r}
km_output <- kmeans(x, centers = 2, nstart = 20)
```

```{r}
km_output$cluster
```

## K-means clustering with K=2
```{r, out.width = "70%", fig.align = "center"}
ggplot(x, aes(x = a, y = b)) +
    geom_point(colour = (km_output$cluster + 1), size = 4)
```


## K-means clustering with K=3
```{r}
set.seed(4)
km_output <- kmeans(x, centers = 3, nstart = 20)
```

## K-means clustering with K=3
```{r, out.width = "70%", fig.align = "center"}
ggplot(x, aes(x = a, y = b)) +
    geom_point(colour = (km_output$cluster+1), size = 4)
```

## Different `nstart` initial cluster assignments result different total within-cluster sum of squares
```{r}
set.seed(3)
km_output <- kmeans(x, centers = 3, nstart = 1)
km_output$tot.withinss
km_output <- kmeans(x, centers = 3, nstart = 5)
km_output$tot.withinss
km_output <- kmeans(x, centers = 3, nstart = 10)
km_output$tot.withinss
```

## How to determine the number of clusters?
1. Run K-means with *K*=1, *K*=2, ..., *K*=*n*.

2. Record total within-cluster sum of squares for each value of *K*.

3. Choose *K* at the *elbow* position.

```{r}
ks <- 1:5
tot_within_ss <- sapply(ks, function(k) {
    km_output <- kmeans(x, k, nstart = 20)
    km_output$tot.withinss
})
```

## How to determine the number of clusters?
```{r, out.width = "90%", fig.align = "center"}
plot(ks, tot_within_ss, type = "b", xlab = "Values of K",
     ylab = "Total within cluster sum of squares")
```

# Hierarchical clustering
- Hierarchical clustering does not require choosing a particular *K* number of clusters.

- It results in a tree-based representation of the observations, called a *dendogram*.

- We focus on *bottom-op* or *agglomerative* clustering (vs *top-down* or *divisive*).

## The dendogram
```{r, out.width = "100%", fig.align = "center", echo=FALSE}
knitr::include_graphics("figures/hierarchical_clust_ex1_2.png")
```

## The dendogram
- Each *leaf* is an observation, as we move up the tree, leafs begin to *fuse* into branches based on their *similarity*.

- The height of fusion (on the vertical axis) indicates how different the observations are.

- Clusters are defined by cuting the dendogram horizontally, although where to make the cut is not so obvious.

- *Hierarchical* refers to that clusters obtained by cutting the tree at a given height are necessarily nested within the clusters obtained by cutting higher.

## Hierarchical clustering algorithm
- We need to define a *dissimilarity measure* between each pair of observations (most often Euclidean distance).

- Starting from the bottom, each observation is treated as a separate cluster, then the two most similar are *fused*, next the two most similar clusters are fused, etc., until all observations belon to a cluster and the dendogram is complete.

- Dissimilarity between clusters depend on the selected *linkage* (average and complete linkage are the most preferred ones) and the dissimilarity measure.

## Linkage types
- The linkage function tells you how to measure the distance between clusters.

- Single linkage: Minimal intercluster dissimilarity.
$$f = min(d(x, y))$$

- Complete linkage: Maximal intercluster dissimilarity.
$$f = max(d(x, y))$$

- Average linkage: Mean intercluster dissimilarity.
$$f = average(d(x, y))$$

## Clustering with different linkages
```{r, out.width = "100%", fig.align = "center", echo=FALSE}
knitr::include_graphics("figures/linkages.png")
```

## Dissimilarity measures
- Euclidean distance is the most common measure used.

$$\sqrt{\sum_{i}(a_{i}-b_{i})^2}$$

- *Correlation-based distance* is also very useful, e.g. it is used for gene expression.
    - It considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.
    - The distance between two vectors is 0 when they are perfectly correlated.
    - It focuses on the shapes of observation profiles rather than their magnitudes.


## Hierachical clustering using different linkage types
```{r}
set.seed(2)
x <- data.table(a = rnorm(50), b = rnorm(50))
x[1:25, `:=`(a = a + 3, b = b - 4)]
```

```{r}
hc_complete <- hclust(dist(x), method = "complete")
hc_average <- hclust(dist(x), method = "average")
hc_single <- hclust(dist(x), method = "single")
```

- Compute the inter-observation Euclidean distance matrix using `dist()`
- Set the linkage type in the `method` argument

## Preparing the dendogram
```{r, eval=FALSE}
par(mfrow = c(1, 3))
plot(hc_complete, main = "Complete linkage", xlab = "",
     ylab = "", sub = "")
plot(hc_average, main = "Average linkage", xlab = "",
     ylab = "", sub = "")
plot(hc_single, main = "Single linkage", xlab = "",
     ylab = "", sub = "")
```

## Preparing the dendogram
```{r, out.width = "90%", fig.align = "center", echo=FALSE}
par(mfrow = c(1, 3))
plot(hc_complete, main = "Complete linkage", xlab = "", ylab = "", sub = "")
plot(hc_average, main = "Average linkage", xlab = "", ylab = "", sub = "")
plot(hc_single, main = "Single linkage", xlab = "", ylab = "", sub = "")
```

## Determining cluster labels with `cutree()` by selecting number of clusters (`k`)
```{r}
cutree(hc_complete, k = 2)
cutree(hc_average, k = 2)
cutree(hc_single, k = 2)
```

## Determining cluster labels with `cutree()` by selecting maximum distance (`h`)
```{r}
cutree(hc_complete, h = 5)
cutree(hc_average, h = 4)
cutree(hc_single, h = 1.4)
```

## Scale variables with `scale()`
```{r, out.width = "80%", fig.align = "center"}
x_scaled <- scale(x)
plot(hclust(dist(x_scaled), method = "complete"),
     main = "Hierarchical clustering with scaled features")
```

## Compute correlation-based distance using `as.dist()`
```{r}
set.seed(5)
x <- matrix(rnorm(30 * 3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
```

## Compute correlation-based distance using `as.dist()`
```{r, out.width = "80%", fig.align = "center"}
plot(hclust(dd, method = "complete"),
     main = "Complete linkage with correlation-based distance",
     xlab = "", sub = "")
```

# Summary
- Supervised vs unsupervised learning

- PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance.

- Clustering looks to find homogeneous subgroups among the observations.
    - In K-means clustering, we seeks to partition the observations into a pre-specified number of clusters.

    - In hierarchical clustering, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a dendrogram, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to *n*.



## Resources
- StatQuest videos:
    + [PCA #1](https://www.youtube.com/watch?v=HMOI_lkzW08&ab_channel=StatQuestwithJoshStarmer)
    + [PCA #2](https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer)
    + [Hierarchical clustering](https://www.youtube.com/watch?v=7xHsRkOdVwo&ab_channel=StatQuestwithJoshStarmer)
    + [K-Means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)
- [James, G., Witten, D., Hastie, T., and Tibshirani, R. *An Introduction to Statistical Learning with Applications in R.*](http://faculty.marshall.usc.edu/gareth-james/ISL/)
